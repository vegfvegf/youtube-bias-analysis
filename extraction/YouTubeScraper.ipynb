{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "#import win_unicode_console\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "#win_unicode_console.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_float(string):\n",
    "    try:\n",
    "        return float(string)\n",
    "    except ValueError:\n",
    "        return -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations (video, search_results, gl, language, recent, loopok, alltime, top_rated):\n",
    "    # Escaping search terms for youtube\n",
    "    escaped_search_terms = urllib.parse.quote(search_term.encode('utf-8'))\n",
    "\n",
    "    # We only want search results that are videos, filtered by viewcoung.\n",
    "    #  This is achieved by using the youtube URI parameter: sp=CAMSAhAB\n",
    "    if alltime:\n",
    "        filter = \"CAMSAhAB\"\n",
    "    else:\n",
    "        if top_rated:\n",
    "            filter = \"CAE%253D\"\n",
    "        else:\n",
    "            filter = \"EgIQAQ%253D%253D\"\n",
    "\n",
    "    url = \"https://www.youtube.com/results?sp=\" + filter + \"&q=\" + escaped_search_terms\n",
    "    if gl:\n",
    "        url = url + '&gl=' + gl\n",
    "\n",
    "    print ('Searching URL: ' + url)\n",
    "\n",
    "    headers = {}\n",
    "    if language:\n",
    "        headers[\"Accept-Language\"] = language\n",
    "    url_request = urllib.request.Request(url, headers=headers)\n",
    "    html = urllib.request.urlopen(url_request)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    if search_results > 20:\n",
    "        print(\"Only 20 results can be returned\")\n",
    "        search_results = 20\n",
    "    \n",
    "    return get_results(soup, search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_from_video(keyword, video, parent_video, search_results, branching, current_depth, depth, name):\n",
    "    # Flag and while required to recover from wrong html received\n",
    "    processed = 1\n",
    "    viewed = []\n",
    "    start = datetime.now()\n",
    "    date = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    while processed > 0:\n",
    "        spacer = \" \" * current_depth * 2\n",
    "        #print(spacer + \"Building tree for: \" + video + \"; Depth: \" + str(current_depth) + \"; at: \" + date)\n",
    "\n",
    "        url = \"https://www.youtube.com/watch?v=\" + video\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                html = urllib.request.urlopen(url)\n",
    "                break\n",
    "            except urllib.error.URLError:\n",
    "                time.sleep(1)\n",
    "        \n",
    "        soup = soup = BeautifulSoup('''\n",
    "            <html> \n",
    "                <h2> Heading 1 </h2> \n",
    "                <h1> Heading 2 </h1> \n",
    "            </html> \n",
    "            ''', \"lxml\") \n",
    "        soup_retry = 0\n",
    "        while soup_retry < 10:\n",
    "            try:\n",
    "                soup = BeautifulSoup(html, \"lxml\")\n",
    "                break\n",
    "#            except urllib.error.URLError:\n",
    "            except:\n",
    "                time.sleep(1)\n",
    "                print('Retrying soup: ' + str(soup_retry))\n",
    "                soup_retry += 1\n",
    "\n",
    "        # Fetch time\n",
    "        fetch_time = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Duration\n",
    "        duration = -1\n",
    "        \n",
    "        for time_count in soup.findAll('meta', {'itemprop': 'duration'}):\n",
    "            try:\n",
    "                dur = time_count['content'].replace('PT', '')\n",
    "                duration = 0\n",
    "                if 'H' in dur:\n",
    "                    contents = dur.split('H')\n",
    "                    duration += int(contents[0]) * 3600\n",
    "                    dur = contents[1]\n",
    "                if 'M' in dur:\n",
    "                    contents = dur.split('M')\n",
    "                    duration += int(contents[0]) * 60\n",
    "                    dur = contents[1]\n",
    "                if 'S' in dur:\n",
    "                    contents = dur.split('S')\n",
    "                    duration += int(contents[0])\n",
    "\n",
    "            except IndexError:\n",
    "                pass\n",
    "        \n",
    "\n",
    "        # Publication date\n",
    "        pubdate = \"\"\n",
    "        for datefield in soup.findAll('meta', {'itemprop': 'datePublished'}):\n",
    "            try:\n",
    "                pubdate = datefield['content']\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        # Channel\n",
    "        channel = ''\n",
    "        for item_section in soup.findAll('script'):\n",
    "            if len(item_section):\n",
    "                script = item_section.string\n",
    "                if \"\\\"url\\\":\\\"/channel/\" in script.string:\n",
    "                    channel = script.split(\"\\\"url\\\":\\\"/channel/\", 1)[-1].split(\"\\\"\", 1)[0]\n",
    "                    break\n",
    "        #if channel == '':\n",
    "            #print ('WARNING: CHANNEL not found')\n",
    "\n",
    "        # Title\n",
    "        title = ''\n",
    "        for eow_title in soup.findAll('title'):\n",
    "            title = eow_title.text.strip()\n",
    "\n",
    "        #if title == '':\n",
    "            #print ('WARNING: title not found')\n",
    "\n",
    "        script = str(soup)\n",
    "        \n",
    "\n",
    "        # Likes\n",
    "        likes = -1\n",
    "        likes_str = script.split(\"{\\\"iconType\\\":\\\"LIKE\\\"},\\\"defaultText\\\":{\\\"accessibility\\\":{\\\"accessibilityData\\\":{\\\"label\\\":\\\"\", 1)[-1] .split(\" \", 1)[0]\n",
    "        if likes_str.isnumeric():\n",
    "            likes = int(likes_str)\n",
    "\n",
    "        # Dislikes\n",
    "        dislikes = -1\n",
    "        dislikes_str = script.split(\"{\\\"iconType\\\":\\\"DISLIKE\\\"},\\\"defaultText\\\":{\\\"accessibility\\\":{\\\"accessibilityData\\\":{\\\"label\\\":\\\"\", 1)[-1] .split(\" \", 1)[0]\n",
    "        if dislikes_str.isnumeric():\n",
    "            dislikes = int(dislikes_str)\n",
    "\n",
    "        # Views\n",
    "        views = -1\n",
    "        views_str = script.split(\"\\\"viewCount\\\":\\\"\", 1)[-1].split(\"\\\"\", 1)[0]\n",
    "        if views_str.isnumeric():\n",
    "            views = int(views_str)\n",
    "\n",
    "        # Width\n",
    "        width = -1\n",
    "        width_str = script.split(\"jpg\\\",\\\"width\\\":\", 1)[-1].split(\",\", 1)[0]\n",
    "        if width_str.isnumeric():\n",
    "            width = int(width_str)\n",
    "\n",
    "        # Height\n",
    "        height = -1\n",
    "        data_str = script.split(\"jpg\\\",\\\"width\\\":\", 1)[-1].split(\",\", 1)[1]\n",
    "        height_str = data_str.split(\"\\\"height\\\":\", 1)[-1].split(\"}\", 1)[0]\n",
    "        if height_str.isnumeric():\n",
    "            height = int(height_str)\n",
    "\n",
    "        # Rating\n",
    "        rating = -1.0\n",
    "        rating_str = script.split(\"averageRating\\\":\", 1)[-1].split(\",\", 1)[0]\n",
    "        rating = return_float(rating_str)\n",
    "\n",
    "        recos = []\n",
    "\n",
    "        #print(spacer + \"Video: \" + video)\n",
    "        #print(\"Duration: \" + str(duration))\n",
    "        #print(\"Publication date: \" + str(pubdate))\n",
    "        #print(\"Channel: \" + channel)\n",
    "        #print(\"Title: \" + title)\n",
    "        #print(\"Likes: \" + str(likes))\n",
    "        #print(\"Dislikes: \" + str(dislikes))\n",
    "        #print(spacer + \"Views: \" + str(views))\n",
    "        #print(spacer + \"Width: \" + str(width))\n",
    "        #print(spacer + \"Height: \" + str(height))\n",
    "        #print(spacer + \"Rating: \" + str(rating))\n",
    "        #print(spacer + \"Current depth: \" + str(current_depth))\n",
    "        \n",
    "\n",
    "        recos_ok = 1\n",
    "        if current_depth < depth:\n",
    "            for item_section in soup.findAll('script'):\n",
    "                if len(item_section):\n",
    "                    script = item_section.string\n",
    "                    if \"\\\"commandMetadata\\\":{\\\"webCommandMetadata\\\":{\\\"url\\\":\\\"/watch?v=\" in script:\n",
    "                        while \"\\\"commandMetadata\\\":{\\\"webCommandMetadata\\\":{\\\"url\\\":\\\"/watch?v=\" in script:\n",
    "                            reco = script.split(\"\\\"commandMetadata\\\":{\\\"webCommandMetadata\\\":{\\\"url\\\":\\\"/watch?v=\", 1)[-1].split(\"\\\"\", 1)[0] #[:11]\n",
    "                            script = script.split(\"\\\"commandMetadata\\\":{\\\"webCommandMetadata\\\":{\\\"url\\\":\\\"/watch?v=\", 1)[-1].split(\"\\\"\", 1)[1]\n",
    "                            if reco not in recos and len(reco) == 11 and reco != video:\n",
    "                                recos.append(reco)\n",
    "\n",
    "            if len(recos) == 0:\n",
    "                #print ('WARNING Could not get a RECOMMENDATION')\n",
    "                recos_ok = 0\n",
    "            else:\n",
    "                recos = recos[0:branching]\n",
    "\n",
    "        #else:\n",
    "            #print(spacer + \"Reached depth: \" + video)\n",
    "\n",
    "        #print(spacer + \"Recos: \")\n",
    "        #print(recos)\n",
    "\n",
    "        reco_count = len(recos)\n",
    "        reco_list = ','.join(recos)\n",
    "\n",
    "        if recos_ok == 1:\n",
    "            end = datetime.now()\n",
    "            elapsed = (end - start).total_seconds()\n",
    "            with open(\"results/\" + name + '.csv', mode='a') as out_file:\n",
    "                result_writer = csv.writer(out_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                result_writer.writerow([keyword, video, parent_video, current_depth, title, width, height, duration, pubdate, channel, likes, dislikes, views, rating, reco_count, fetch_time, reco_list, elapsed])\n",
    "\n",
    "            if title == '' or width == -1 or height == -1 or duration == -1 \\\n",
    "                or pubdate == '' or channel == '' or likes == -1 or dislikes == -1 \\\n",
    "                or views == -1 or rating == -1.0:\n",
    "                    date = time.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "                    data_filename = \"data/\" + name + \"_data_\" + video + '_' + parent_video + \"_\" + str(current_depth) + '_' + date + \".html\"\n",
    "                    #with open(data_filename, \"w\") as data_file:\n",
    "                        #data_file.write(str(soup.prettify))\n",
    "            processed = 0\n",
    "            current_depth += 1\n",
    "            for reco in recos:\n",
    "                build_tree_from_video(keyword, reco, video, search_results, branching, current_depth, depth, name)\n",
    "        else:\n",
    "            processed += 1\n",
    "            # Time to avoid being received the same wrong html\n",
    "            time.sleep(processed * 1)\n",
    "            if processed == 10:\n",
    "                print(\"Giving-up, retried 10 times\")\n",
    "                date = time.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "                fail_filename = \"fail/\" + name + \"_fail_\" + video + \"_\" + str(processed) + \"_\" + parent_video + \"_\" + str(current_depth) + '_' + date + \".html\"\n",
    "                print(spacer + \"Processing NOT succesful; Retry no.: \" + str(processed) + \" Fail file name: \" + fail_filename)\n",
    "                #with open(fail_filename, \"w\") as file:\n",
    "                    #file.write(str(soup.prettify))\n",
    "            current_depth += 1\n",
    "\n",
    "        #\"views\": \"408841\",\n",
    "        #\"likes\": 6449,\n",
    "        #\"dislikes\": 262,\n",
    "        #\"recommendations\": [\"KBoXtp8GIp8\", \"OBlOJFoOTmw\", \"sZDtyUAK0x8\", \"4bnO_agdHGo\", \"pBIOkvC6oXI\", \"aJ8lukZYrPI\", \"2mSURwkDk28\", \"7oxlCKMlpZw\", \"7UaIEaVqolU\", \"rD7lARdorUA\", \"e2P2GoVJS28\", \"H3iQbrzf-4g\", \"uOBF9qPto4c\", \"tTdAQAT80aM\", \"gWVHses2GCY\", \"lHos5r2cvPI\", \"dt5heT2_XaE\", \"DJuPwtnQHFY\", \"TAOretPBFaM\", \"8r6yV78XIzQ\", \"6d8IpcCBHtM\", \"40dnEuHWySU\", \"nlPpV5h2LhI\", \"99zXTK0aKoU\"],\n",
    "        #\"title\": \"Trump\\u2019s COVID-19 Vaccine Outburst, Prince William Defends Royal Family | The Tonight Show - YouTube\",\n",
    "        #\"depth\": 1, \"id\": \"aJ8lukZYrPI\",\n",
    "        #\"channel\": \"UC8-Th83bH_thdKZDJCrn88g\",\n",
    "        #\"pubdate\": \"2021-03-11\",\n",
    "        #\"duration\": 305,\n",
    "        #\"key\": [],\n",
    "        #\"nb_recommendations\": 2,\n",
    "        #\"mult\": 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(soup, search_results, name):\n",
    "    videos = []\n",
    "    # Modified by VGF - YouTube has moved the href into JavaScript and can only be found under <script>\n",
    "    #for item_section in soup.findAll('div', {'class': 'yt-lockup-dismissable'}):\n",
    "        #video = item_section.contents[0].contents[0]['href'].split('=')[1]\n",
    "        #videos.append(video)\n",
    "\n",
    "    for item_section in soup.findAll('script'):\n",
    "        if len(item_section):\n",
    "            script=str(item_section.string)\n",
    "            if \"watch?v=\" in script:\n",
    "                while \"watch?v=\" in script:\n",
    "                    video = script.split(\"watch?v=\", 1)[-1].split(\"\\\"\", 1)[0]\n",
    "                    script = script.split(\"watch?v=\", 1)[-1].split(\"\\\"\", 1)[1]\n",
    "                    videos.append(video)\n",
    "    if videos == []:\n",
    "        date = time.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "        data_filename = \"data/\" + name + \"_NoSearchREsults_\" + date + \".html\"\n",
    "        #with open(data_filename, \"w\") as data_file:\n",
    "            #data_file.write(str(soup.prettify))\n",
    "\n",
    "    return videos[0:search_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_term (search_term, search_results, gl, language, recent, loopok, alltime, top_rated, name):\n",
    "    # Escaping search terms for youtube\n",
    "    escaped_search_terms = urllib.parse.quote(search_term.encode('utf-8'))\n",
    "\n",
    "    # We only want search results that are videos, filtered by viewcoung.\n",
    "    #  This is achieved by using the youtube URI parameter: sp=CAMSAhAB\n",
    "    if alltime:\n",
    "        filter = \"CAMSAhAB\"\n",
    "    else:\n",
    "        if top_rated:\n",
    "            filter = \"CAE%253D\"\n",
    "        else:\n",
    "            filter = \"EgIQAQ%253D%253D\"\n",
    "\n",
    "    url = \"https://www.youtube.com/results?sp=\" + filter + \"&q=\" + escaped_search_terms\n",
    "    if gl:\n",
    "        url = url + '&gl=' + gl\n",
    "\n",
    "    print ('Searching URL: ' + url)\n",
    "\n",
    "    headers = {}\n",
    "    if language:\n",
    "        headers[\"Accept-Language\"] = language\n",
    "    url_request = urllib.request.Request(url, headers=headers)\n",
    "    html = urllib.request.urlopen(url_request)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    if search_results > 20:\n",
    "        print('Only 20 results can be returned')\n",
    "        search_results = 20\n",
    "    \n",
    "    return get_results(soup, search_results, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_from_search(query, search_results, branching, depth, gl, language, recent, loopok, alltime, top_rated):\n",
    "    \"\"\"\n",
    "        Splits the query into keywords around commas and runs a scrapping for each keyword\n",
    "    \"\"\"\n",
    "    keywords = query.split(',')\n",
    "    date = time.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "    print('Start: ' + date)\n",
    "\n",
    "    top_videos = {}\n",
    "    for keyword in query.split(','):\n",
    "        file_name = keyword.replace(' ', '') + \"_\" + str(search_results) + \"_\" + str(branching) + \"_\" + str(depth) + '-' + date\n",
    "    \n",
    "        print('Running, will save the resulting to: ' + file_name)\n",
    "\n",
    "        with open('results/' + file_name + '.csv', mode='w') as out_file:\n",
    "            result_writer = csv.writer(out_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            result_writer.writerow(['keyword', 'video', 'parent_video', 'current_depth', 'title', 'width', 'height', 'duration', 'pubdate', 'channel', 'likes', 'dislikes', 'viewes', 'rating', 'reco_count', 'fetch_time', 'reco_list', 'elapsed'])\n",
    "\n",
    "        top_search_resutls = search_for_term(keyword, search_results, gl, language, recent, loopok, alltime, top_rated, file_name)\n",
    "        print(\"*************************Search resutls for: \" + keyword)\n",
    "        print(top_search_resutls)\n",
    "\n",
    "        for video in top_search_resutls:\n",
    "            build_tree_from_video(keyword, video, \"\", search_results, branching, 0, depth, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2021-06-12_05:00:14\n",
      "Running, will save the resulting to: trumpwasthebestpresidentever_5_5_5-2021-06-12_05:00:14\n",
      "Searching URL: https://www.youtube.com/results?sp=EgIQAQ%253D%253D&q=trump%20was%20the%20best%20president%20ever&gl=US\n",
      "*************************Search resutls for: trump was the best president ever\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#build_tree_from_search(\"Joe Biden,Bill Clinton,Hillary Clinton\", 5, 4, 3, \"US\", \"en\", False, False, False, False)\n",
    "build_tree_from_search(\"trump was the best president ever\", 5, 5, 5, \"US\", \"en\", False, False, False, False)\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
