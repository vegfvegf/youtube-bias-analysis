{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "#import win_unicode_console\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "#win_unicode_console.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_float(string):\n",
    "    try:\n",
    "        return float(string)\n",
    "    except ValueError:\n",
    "        return -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations (video, search_results, gl, language, recent, loopok, alltime, top_rated):\n",
    "    # Escaping search terms for youtube\n",
    "    escaped_search_terms = urllib.parse.quote(search_term.encode('utf-8'))\n",
    "\n",
    "    # We only want search results that are videos, filtered by viewcoung.\n",
    "    #  This is achieved by using the youtube URI parameter: sp=CAMSAhAB\n",
    "    if alltime:\n",
    "        filter = \"CAMSAhAB\"\n",
    "    else:\n",
    "        if top_rated:\n",
    "            filter = \"CAE%253D\"\n",
    "        else:\n",
    "            filter = \"EgIQAQ%253D%253D\"\n",
    "\n",
    "    url = \"https://www.youtube.com/results?sp=\" + filter + \"&q=\" + escaped_search_terms\n",
    "    if gl:\n",
    "        url = url + '&gl=' + gl\n",
    "\n",
    "    print ('Searching URL: ' + url)\n",
    "\n",
    "    headers = {}\n",
    "    if language:\n",
    "        headers[\"Accept-Language\"] = language\n",
    "    url_request = urllib.request.Request(url, headers=headers)\n",
    "    html = urllib.request.urlopen(url_request)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    if search_results > 20:\n",
    "        print(\"Only 20 results can be returned\")\n",
    "        search_results = 20\n",
    "    \n",
    "    return get_results(soup, search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_from_video(keyword, video, parent_video, search_results, branching, current_depth, depth, name, trace, parent_trace):\n",
    "    # Flag and while required to recover from wrong html received\n",
    "#    print (keyword, video, trace)\n",
    "    processed = 1\n",
    "    viewed = []\n",
    "    start = datetime.now()\n",
    "    date = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    while processed > 0:\n",
    "        spacer = \" \" * current_depth * 2\n",
    "\n",
    "        url = \"https://www.youtube.com/watch?v=\" + video\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                html = urllib.request.urlopen(url)\n",
    "                break\n",
    "            except urllib.error.URLError:\n",
    "                time.sleep(1)\n",
    "        \n",
    "        soup = soup = BeautifulSoup('''\n",
    "            <html> \n",
    "                <h2> Heading 1 </h2> \n",
    "                <h1> Heading 2 </h1> \n",
    "            </html> \n",
    "            ''', \"lxml\") \n",
    "        soup_retry = 0\n",
    "        while soup_retry < 10:\n",
    "            try:\n",
    "                soup = BeautifulSoup(html, \"lxml\")\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(1)\n",
    "                #print('Retrying soup: ' + str(soup_retry))\n",
    "                soup_retry += 1\n",
    "\n",
    "        # Fetch time\n",
    "        fetch_time = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Publication date\n",
    "        pubdate = \"\"\n",
    "        for datefield in soup.findAll('meta', {'itemprop': 'datePublished'}):\n",
    "            try:\n",
    "                pubdate = datefield['content']\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        script = str(soup)\n",
    "\n",
    "        # Likes\n",
    "        likes = -1\n",
    "        likes_str = script.split(\"{\\\"iconType\\\":\\\"LIKE\\\"},\\\"defaultText\\\":{\\\"accessibility\\\":{\\\"accessibilityData\\\":{\\\"label\\\":\\\"\", 1)[-1] .split(\" \", 1)[0]\n",
    "        if likes_str.isnumeric():\n",
    "            likes = int(likes_str)\n",
    "\n",
    "        # Dislikes\n",
    "        dislikes = -1\n",
    "        dislikes_str = script.split(\"{\\\"iconType\\\":\\\"DISLIKE\\\"},\\\"defaultText\\\":{\\\"accessibility\\\":{\\\"accessibilityData\\\":{\\\"label\\\":\\\"\", 1)[-1] .split(\" \", 1)[0]\n",
    "        if dislikes_str.isnumeric():\n",
    "            dislikes = int(dislikes_str)\n",
    "\n",
    "        # Rating\n",
    "        rating = -1.0\n",
    "        rating_str = script.split(\"averageRating\\\":\", 1)[-1].split(\",\", 1)[0]\n",
    "        rating = return_float(rating_str)\n",
    "\n",
    "        recos = []\n",
    "\n",
    "        recos_ok = 1\n",
    "        if current_depth < depth:\n",
    "            for item_section in soup.findAll('script'):\n",
    "                if len(item_section):\n",
    "                    script = item_section.string\n",
    "                    if \"\\\"commandMetadata\\\":{\\\"webCommandMetadata\\\":{\\\"url\\\":\\\"/watch?v=\" in script:\n",
    "                        while \"\\\"commandMetadata\\\":{\\\"webCommandMetadata\\\":{\\\"url\\\":\\\"/watch?v=\" in script:\n",
    "                            reco = script.split(\"\\\"commandMetadata\\\":{\\\"webCommandMetadata\\\":{\\\"url\\\":\\\"/watch?v=\", 1)[-1].split(\"\\\"\", 1)[0] #[:11]\n",
    "                            script = script.split(\"\\\"commandMetadata\\\":{\\\"webCommandMetadata\\\":{\\\"url\\\":\\\"/watch?v=\", 1)[-1].split(\"\\\"\", 1)[1]\n",
    "                            if reco not in recos and len(reco) == 11 and reco != video:\n",
    "                                recos.append(reco)\n",
    "\n",
    "            if len(recos) == 0:\n",
    "                #print ('WARNING Could not get a RECOMMENDATION')\n",
    "                recos_ok = 0\n",
    "            else:\n",
    "                recos = recos[0:branching]\n",
    "\n",
    "        reco_count = len(recos)\n",
    "        reco_list = ','.join(recos)\n",
    "\n",
    "        if recos_ok == 1:\n",
    "            end = datetime.now()\n",
    "            elapsed = (end - start).total_seconds()\n",
    "            with open(\"results/\" + name + '.csv', mode='a') as out_file:\n",
    "                result_writer = csv.writer(out_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                result_writer.writerow([keyword, video, parent_video, current_depth, pubdate, likes, dislikes, rating, reco_count, fetch_time, reco_list, elapsed, trace, parent_trace])\n",
    "\n",
    "            processed = 0\n",
    "            trace_seed = 1\n",
    "            current_depth += 1\n",
    "            for reco in recos:\n",
    "                childTrace = trace + ((10 ** (4 - current_depth + 1)) * trace_seed)\n",
    "                build_tree_from_video(keyword, reco, video, search_results, branching, current_depth, depth, name, childTrace, trace)\n",
    "                trace_seed += 1\n",
    "        else:\n",
    "            processed += 1\n",
    "            # Time to avoid being received the same wrong html\n",
    "            time.sleep(processed * 1)\n",
    "            if processed == 5:\n",
    "                print(\"Giving-up, retried 10 times\")\n",
    "                date = time.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "                fail_filename = \"fail/\" + name + \"_fail_\" + video + \"_\" + str(processed) + \"_\" + parent_video + \"_\" + str(current_depth) + '_' + date + \".html\"\n",
    "                print(spacer + \"Processing NOT succesful; Retry no.: \" + str(processed) + \" Fail file name: \" + fail_filename)\n",
    "                #with open(fail_filename, \"w\") as file:\n",
    "                    #file.write(str(soup.prettify))\n",
    "            current_depth += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(soup, search_results, name):\n",
    "    videos = []\n",
    "    # Modified by VGF - YouTube has moved the href into JavaScript and can only be found under <script>\n",
    "    #for item_section in soup.findAll('div', {'class': 'yt-lockup-dismissable'}):\n",
    "        #video = item_section.contents[0].contents[0]['href'].split('=')[1]\n",
    "        #videos.append(video)\n",
    "\n",
    "    for item_section in soup.findAll('script'):\n",
    "        if len(item_section):\n",
    "            script=str(item_section.string)\n",
    "            if \"watch?v=\" in script:\n",
    "                while \"watch?v=\" in script:\n",
    "                    video = script.split(\"watch?v=\", 1)[-1].split(\"\\\"\", 1)[0]\n",
    "                    script = script.split(\"watch?v=\", 1)[-1].split(\"\\\"\", 1)[1]\n",
    "                    videos.append(video)\n",
    "    if videos == []:\n",
    "        date = time.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "        data_filename = \"data/\" + name + \"_NoSearchREsults_\" + date + \".html\"\n",
    "        #with open(data_filename, \"w\") as data_file:\n",
    "            #data_file.write(str(soup.prettify))\n",
    "\n",
    "    return videos[0:search_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_term (search_term, search_results, gl, language, recent, loopok, alltime, top_rated, name):\n",
    "    # Escaping search terms for youtube\n",
    "    escaped_search_terms = urllib.parse.quote(search_term.encode('utf-8'))\n",
    "\n",
    "    # We only want search results that are videos, filtered by viewcoung.\n",
    "    #  This is achieved by using the youtube URI parameter: sp=CAMSAhAB\n",
    "    if alltime:\n",
    "        filter = \"CAMSAhAB\"\n",
    "    else:\n",
    "        if top_rated:\n",
    "            filter = \"CAE%253D\"\n",
    "        else:\n",
    "            filter = \"EgIQAQ%253D%253D\"\n",
    "\n",
    "    url = \"https://www.youtube.com/results?sp=\" + filter + \"&q=\" + escaped_search_terms\n",
    "    if gl:\n",
    "        url = url + '&gl=' + gl\n",
    "\n",
    "    print ('Searching URL: ' + url)\n",
    "\n",
    "    headers = {}\n",
    "    if language:\n",
    "        headers[\"Accept-Language\"] = language\n",
    "    url_request = urllib.request.Request(url, headers=headers)\n",
    "    html = urllib.request.urlopen(url_request)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    if search_results > 20:\n",
    "        print('Only 20 results can be returned')\n",
    "        search_results = 20\n",
    "    \n",
    "    return get_results(soup, search_results, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_from_search(query, search_results, branching, depth, gl, language, recent, loopok, alltime, top_rated):\n",
    "    \"\"\"\n",
    "        Splits the query into keywords around commas and runs a scrapping for each keyword\n",
    "    \"\"\"\n",
    "    keywords = query.split(',')\n",
    "    date = time.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "    print('Start: ' + date)\n",
    "\n",
    "    top_videos = {}\n",
    "    \n",
    "    trace_seed = 1\n",
    "    for keyword in query.split(','):\n",
    "        \n",
    "        trace = trace_seed * 1000000\n",
    "        \n",
    "        file_name = keyword.replace(' ', '') + \"_\" + str(search_results) + \"_\" + str(branching) + \"_\" + str(depth) + '-' + date\n",
    "    \n",
    "        print('Running, will save the resulting to: ' + file_name)\n",
    "\n",
    "        with open('results/' + file_name + '.csv', mode='w') as out_file:\n",
    "            result_writer = csv.writer(out_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            result_writer.writerow(['keyword', 'video', 'parent_video', 'current_depth', 'pubdate', 'likes', 'dislikes', 'rating', 'reco_count', 'fetch_time', 'reco_list', 'elapsed', 'trace', 'parent_trace'])\n",
    "\n",
    "        top_search_resutls = search_for_term(keyword, search_results, gl, language, recent, loopok, alltime, top_rated, file_name)\n",
    "        print(\"*************************Search resutls for: \" + keyword)\n",
    "        print(top_search_resutls)\n",
    "\n",
    "        for video in top_search_resutls:\n",
    "            build_tree_from_video(keyword, video, \"\", search_results, branching, 0, depth, file_name, trace, 0)\n",
    "        \n",
    "        trace_seed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_tree_from_search(\"Joe Biden,Hillary Clinton,Donald Trump,AOC,Lets go Brandon\", 5, 5, 5, \"US\", \"en-US\", False, False, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_tree_from_search(\"Lets go Brandon,Kyle Rittenhouse,Joseph Rosenbaum,Anthony Huber,Ilhan Omar\", 5, 5, 5, \"US\", \"en-US\", False, False, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_tree_from_search(\"Lindsey Graham,Mitch McConnell,Nancy Pelosi,Newt Gingrich,Barak Obama,Ted Cruz\", 5, 5, 5, \"US\", \"en-US\", False, False, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_tree_from_search(\"Climate Change,Cultural Appropriation,Feminism,Free Speech,Gun Control\", 5, 5, 5, \"US\", \"en-US\", False, False, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_tree_from_search(\"Joe Manchin,Immigration,Moderna,Pfizer,NRA,Mask,Planned Parenthood,Vaccine,Welfare State\", 5, 5, 5, \"US\", \"en-US\", False, False, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_tree_from_search(\"Infrastructure Bill,Anthony Fauci,Kyrie Irving,Vaxxed,Omicron,Gavin Newson,Peter Thiel,trump was the best president ever,trump was the worst president ever,biden is the best president ever,biden is the worst president ever\", 5, 5, 5, \"US\", \"en-US\", False, False, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_tree_from_search(\"trump was the best president ever\", 5, 5, 5, \"US\", \"en\", False, False, False, False)\n",
    "#build_tree_from_search(\"trump was the worst president ever\", 5, 5, 5, \"US\", \"en\", False, False, False, False)\n",
    "#build_tree_from_search(\"biden is the best president ever\", 5, 5, 5, \"US\", \"en\", False, False, False, False)\n",
    "#build_tree_from_search(\"biden is the worst president ever\", 5, 5, 5, \"US\", \"en\", False, False, False, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2022-01-23_10:27:07\n",
      "Running, will save the resulting to: COVID_5_5_5-2022-01-23_10:27:07\n",
      "Searching URL: https://www.youtube.com/results?sp=EgIQAQ%253D%253D&q=COVID&gl=US\n",
      "*************************Search resutls for: COVID\n",
      "['mXVpH5SyHGM', 'LjwIXFV_1j4', 'pu7PtzUUcz0', 'mZlyPFP0ilM', 'SzSFRWNzuOU']\n",
      "Giving-up, retried 10 times\n",
      "        Processing NOT succesful; Retry no.: 5 Fail file name: fail/COVID_5_5_5-2022-01-23_10:27:07_fail_Kh2vWO58sj4_5_LjwIXFV_1j4_4_2022-01-23_16:24:29.html\n"
     ]
    }
   ],
   "source": [
    "build_tree_from_search(\"COVID,Coronavirus\", 5, 5, 5, \"US\", \"en-US\", False, False, False, False)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
